{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10dc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterator, Sequence\n",
    "import multiprocessing\n",
    "import os\n",
    "import typing\n",
    "from typing import Protocol, SupportsIndex, TypeVar\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import lerobot.common.datasets.lerobot_dataset as lerobot_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import openpi.models.model as _model\n",
    "import openpi.training.config as _config\n",
    "import openpi.transforms as _transforms\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from pathlib import Path\n",
    "\n",
    "# class AgiBotDataset(LeRobotDataset):\n",
    "#     @classmethod\n",
    "#     def create(\n",
    "#         cls,\n",
    "#         repo_id: str,\n",
    "#         fps: int,\n",
    "#         features: dict,\n",
    "#         root: str | Path | None = None,\n",
    "#         robot_type: str | None = None,\n",
    "#         use_videos: bool = False,\n",
    "#         tolerance_s: float = 1e-4,\n",
    "#         image_writer_processes: int = 0,\n",
    "#         image_writer_threads: int = 0,\n",
    "#         video_backend: str | None = None,\n",
    "#         image_path: str | None = None\n",
    "#     ) -> \"LeRobotDataset\":\n",
    "#         \"\"\"Create a agibotworld LeRobot Dataset from scratch in order to record data.\"\"\"\n",
    "#         obj = cls.__new__(cls)\n",
    "#         obj.meta = LeRobotDatasetMetadata.create(\n",
    "#             repo_id=repo_id,\n",
    "#             fps=fps,\n",
    "#             robot_type=robot_type,\n",
    "#             features=features,\n",
    "#             root=root,\n",
    "#             use_videos=use_videos\n",
    "#         )\n",
    "#         obj.repo_id = obj.meta.repo_id\n",
    "#         obj.root = obj.meta.root\n",
    "#         obj.revision = None\n",
    "#         obj.tolerance_s = tolerance_s\n",
    "#         obj.image_writer = None\n",
    "\n",
    "#         if image_writer_processes or image_writer_threads:\n",
    "#             obj.start_image_writer(image_writer_processes, image_writer_threads)\n",
    "\n",
    "#         obj.episode_buffer = obj.create_episode_buffer()\n",
    "\n",
    "#         obj.episodes = None\n",
    "#         obj.hf_dataset = obj.create_hf_dataset()\n",
    "#         obj.image_transforms = None\n",
    "#         obj.delta_timestamps = None\n",
    "#         obj.delta_indices = None\n",
    "#         obj.episode_data_index = None\n",
    "#         obj.image_path = Path(image_path) if image_path else None\n",
    "#         return obj\n",
    "\n",
    "#     def __getitem__(self, idx: int) -> dict:\n",
    "#         \"\"\"Override the parent __getitem__ to include image loading from file paths.\"\"\"\n",
    "#         # Get the base item from parent class\n",
    "#         item = super().__getitem__(idx)\n",
    "        \n",
    "#         # Load images from file paths and add to item\n",
    "#         image_data = self._query_images(None, idx)\n",
    "        \n",
    "#         # Map the loaded images to the expected format\n",
    "#         if image_data:\n",
    "#             if \"observation.images\" not in item:\n",
    "#                 item[\"observation.images\"] = {}\n",
    "            \n",
    "#             # Map camera names to expected keys\n",
    "#             camera_mapping = {\n",
    "#                 \"observation.images.head_color\": \"head\",\n",
    "#                 \"observation.images.hand_left_color\": \"hand_left\", \n",
    "#                 \"observation.images.hand_right_color\": \"hand_right\"\n",
    "#             }\n",
    "            \n",
    "#             for camera_key, image_tensor in image_data.items():\n",
    "#                 if camera_key in camera_mapping:\n",
    "#                     expected_key = camera_mapping[camera_key]\n",
    "#                     item[\"observation.images\"][expected_key] = image_tensor\n",
    "        \n",
    "#         return item\n",
    "\n",
    "#     def _query_images(self, query_indices: dict[str, list[int]] | None, idx: int) -> dict[str, torch.Tensor]:\n",
    "#         \"\"\"Load images directly from file paths constructed from observation.images.path data.\"\"\"\n",
    "#         item = {}\n",
    "\n",
    "#         # Check if we have observation.images.path in the dataset\n",
    "#         if \"observation.images.path\" not in self.hf_dataset.column_names:\n",
    "#             return item\n",
    "\n",
    "#         if query_indices is not None:\n",
    "#             # Load multiple images for delta timestamps\n",
    "#             if \"observation.images.path\" in query_indices:\n",
    "#                 selected_data = self.hf_dataset.select(query_indices[\"observation.images.path\"])\n",
    "#                 path_arrays = selected_data[\"observation.images.path\"]\n",
    "\n",
    "#                 all_images = {}\n",
    "#                 for camera_name in [\"hand_left_color\", \"hand_right_color\", \"head_color\"]:\n",
    "#                     images = []\n",
    "#                     for path_array in path_arrays:\n",
    "#                         img_path = self._construct_image_path(path_array, camera_name)\n",
    "#                         image = self._load_single_image(img_path)\n",
    "#                         images.append(image)\n",
    "#                     all_images[f\"observation.images.{camera_name}\"] = torch.stack(images)\n",
    "#                 item.update(all_images)\n",
    "#         else:\n",
    "#             # Load single image for current timestamp\n",
    "#             path_array = self.hf_dataset[idx][\"observation.images.path\"]\n",
    "\n",
    "#             # Load all camera images for current timestamp\n",
    "#             for camera_name in [\"hand_left_color\", \"hand_right_color\", \"head_color\"]:\n",
    "#                 img_path = self._construct_image_path(path_array, camera_name)\n",
    "#                 item[f\"observation.images.{camera_name}\"] = self._load_single_image(img_path)\n",
    "#         item['prompt'] = \" \"\n",
    "#         return item\n",
    "\n",
    "#     def _construct_image_path(self, path_array: list | torch.Tensor, camera_name: str) -> Path:\n",
    "#         \"\"\"Construct image path from path array and camera name.\"\"\"\n",
    "#         if isinstance(path_array, torch.Tensor):\n",
    "#             path_array = path_array.tolist()\n",
    "\n",
    "#         task_id, job_id, episode_id, frame_index = path_array\n",
    "#         sn_code = \"A2D0015AB00061\"\n",
    "\n",
    "#         extension = '.png' if camera_name.endswith('_depth') else '.jpg'\n",
    "\n",
    "#         img_path = (self.image_path /\n",
    "#                    str(task_id) /\n",
    "#                    str(job_id) /\n",
    "#                    sn_code /\n",
    "#                    str(episode_id) /\n",
    "#                    \"camera\" /\n",
    "#                    str(frame_index) /\n",
    "#                    f\"{camera_name}{extension}\")\n",
    "\n",
    "#         return img_path\n",
    "    \n",
    "#     def _load_single_image(self, img_path: Path) -> torch.Tensor:\n",
    "#         \"\"\"Load a single image and convert to torch tensor.\"\"\"\n",
    "#         import PIL.Image\n",
    "\n",
    "#         # Check if file exists\n",
    "#         if not img_path.exists():\n",
    "#             raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "\n",
    "#         # Load image\n",
    "#         image = PIL.Image.open(img_path).convert('RGB')\n",
    "\n",
    "#         # Convert to numpy array and then to tensor [C, H, W] with values in [0, 1]\n",
    "#         image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "#         image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)\n",
    "\n",
    "#         return image_tensor\n",
    "\n",
    "#     def _query_hf_dataset(self, query_indices: dict[str, list[int]]) -> dict:\n",
    "#         \"\"\"Override to exclude image path from normal querying.\"\"\"\n",
    "#         return {\n",
    "#             key: torch.stack(self.hf_dataset.select(q_idx)[key])\n",
    "#             for key, q_idx in query_indices.items()\n",
    "#             if key not in self.meta.video_keys and key != \"observation.images.path\"\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ff655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpi.training.config as _config\n",
    "_config.TrainConfig.image_path = \"/dataset/SimDatas/\"\n",
    "config =  _config.TrainConfig\n",
    "import dataclasses\n",
    "import functools\n",
    "import logging\n",
    "import platform\n",
    "from typing import Any\n",
    "\n",
    "import etils.epath as epath\n",
    "import flax.nnx as nnx\n",
    "from flax.training import common_utils\n",
    "import flax.traverse_util as traverse_util\n",
    "import jax\n",
    "import jax.experimental\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tqdm_loggable.auto as tqdm\n",
    "import wandb\n",
    "\n",
    "import openpi.models.model as _model\n",
    "import openpi.shared.array_typing as at\n",
    "import openpi.shared.nnx_utils as nnx_utils\n",
    "import openpi.training.checkpoints as _checkpoints\n",
    "import openpi.training.config as _config\n",
    "import openpi.training.data_loader as _data_loader\n",
    "import openpi.training.optimizer as _optimizer\n",
    "import openpi.training.sharding as sharding\n",
    "import openpi.training.utils as training_utils\n",
    "import openpi.training.weight_loaders as _weight_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43bf91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainConfig(name='pi0_base_agibot_lora', project_name='openpi', exp_name=<tyro._singleton.PropagatingMissingType object at 0x78477814a410>, model=Pi0Config(action_dim=32, action_horizon=50, max_token_len=48, dtype='bfloat16', paligemma_variant='gemma_2b_lora', action_expert_variant='gemma_300m_lora'), weight_loader=CheckpointWeightLoader(params_path='s3://openpi-assets/checkpoints/pi0_base/params'), lr_schedule=CosineDecaySchedule(warmup_steps=1000, peak_lr=2.5e-05, decay_steps=30000, decay_lr=2.5e-06), optimizer=AdamW(b1=0.9, b2=0.95, eps=1e-08, weight_decay=1e-10, clip_gradient_norm=1.0), ema_decay=0.99, freeze_filter=All(PathRegex(pattern=re.compile('.*llm.*'), sep='/'), Not(PathRegex(pattern=re.compile('.*lora.*'), sep='/'))), data=LeRobotAgiBotDataConfig(repo_id='/dataset/lerobot_dataset', assets=AssetsConfig(assets_dir=None, asset_id=None), base_config=DataConfig(repo_id=None, asset_id=None, norm_stats=None, repack_transforms=Group(inputs=(), outputs=()), data_transforms=Group(inputs=(), outputs=()), model_transforms=Group(inputs=(), outputs=()), use_quantile_norm=False, action_sequence_keys=('actions',), prompt_from_task=True, local_files_only=True), use_delta_joint_actions=True, default_prompt=None, adapt_to_pi=False, repack_transforms=Group(inputs=[RepackTransform(structure={'state': 'observation.states', 'actions': 'actions'})], outputs=()), action_sequence_keys=('actions',)), assets_base_dir='./assets', checkpoint_base_dir='./checkpoints/', seed=42, batch_size=32, num_workers=2, num_train_steps=10000, log_interval=100, save_interval=1000, keep_period=5000, overwrite=False, resume=False, wandb_enabled=True, policy_metadata=None, fsdp_devices=1, image_path='/dataset/SimDatas/')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import openpi.training.data_loader as _data_loader\n",
    "config=_config.get_config(\"pi0_base_agibot_lora\")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4491d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cd /dataset/SimDatas/ && ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d26bbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_data=LeRobotDatasetMetadata({\n",
      "    Repository ID: '',\n",
      "    Total episodes: '11',\n",
      "    Total frames: '6032',\n",
      "    Features: '['observation.images.path', 'observation.states', 'actions', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index']',\n",
      "})',\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jax.config.update(\"jax_compilation_cache_dir\", str(epath.Path(\"~/.cache/jax\").expanduser()))\n",
    "\n",
    "mesh = sharding.make_mesh(config.fsdp_devices)\n",
    "data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(sharding.DATA_AXIS))\n",
    "\n",
    "data_loader = _data_loader.create_data_loader(\n",
    "        config,\n",
    "        sharding=data_sharding,\n",
    "        num_workers=config.num_workers,\n",
    "        shuffle=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba98657",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/app/src/openpi/training/data_loader.py\", line 207, in __getitem__\n    return self._transform(self._dataset[index])\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/src/openpi/transforms.py\", line 75, in __call__\n    data = transform(data)\n           ^^^^^^^^^^^^^^^\n  File \"/app/src/openpi/policies/agibot_policy.py\", line 51, in __call__\n    data = _decode_agibot(data, adapt_to_pi=self.adapt_to_pi)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/src/openpi/policies/agibot_policy.py\", line 167, in _decode_agibot\n    observation = data[\"observation\"]\n                  ~~~~^^^^^^^^^^^^^^^\nKeyError: 'observation'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(data_loader)\n\u001b[0;32m----> 2\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized data loader:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtraining_utils\u001b[38;5;241m.\u001b[39marray_tree_to_info(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/app/src/openpi/training/data_loader.py:339\u001b[0m, in \u001b[0;36mcreate_data_loader.<locals>.DataLoaderImpl.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mObservation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/app/src/openpi/training/data_loader.py:421\u001b[0m, in \u001b[0;36mTorchDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# We've exhausted the dataset. Create a new iterator and start over.\u001b[39;00m\n",
      "File \u001b[0;32m/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/.venv/lib/python3.11/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/app/src/openpi/training/data_loader.py\", line 207, in __getitem__\n    return self._transform(self._dataset[index])\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/src/openpi/transforms.py\", line 75, in __call__\n    data = transform(data)\n           ^^^^^^^^^^^^^^^\n  File \"/app/src/openpi/policies/agibot_policy.py\", line 51, in __call__\n    data = _decode_agibot(data, adapt_to_pi=self.adapt_to_pi)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/src/openpi/policies/agibot_policy.py\", line 167, in _decode_agibot\n    observation = data[\"observation\"]\n                  ~~~~^^^^^^^^^^^^^^^\nKeyError: 'observation'\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(data_loader)\n",
    "batch = next(data_iter)\n",
    "logging.info(f\"Initialized data loader:\\n{training_utils.array_tree_to_info(batch)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
