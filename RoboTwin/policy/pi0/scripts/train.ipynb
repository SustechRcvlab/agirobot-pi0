{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c56209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import functools\n",
    "import logging\n",
    "import platform\n",
    "from typing import Any\n",
    "\n",
    "import etils.epath as epath\n",
    "import flax.nnx as nnx\n",
    "from flax.training import common_utils\n",
    "import flax.traverse_util as traverse_util\n",
    "import jax\n",
    "import jax.experimental\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import tqdm_loggable.auto as tqdm\n",
    "import wandb\n",
    "\n",
    "import openpi.models.model as _model\n",
    "import openpi.shared.array_typing as at\n",
    "import openpi.shared.nnx_utils as nnx_utils\n",
    "import openpi.training.checkpoints as _checkpoints\n",
    "import openpi.training.config as _config\n",
    "import openpi.training.data_loader as _data_loader\n",
    "import openpi.training.optimizer as _optimizer\n",
    "import openpi.training.sharding as sharding\n",
    "import openpi.training.utils as training_utils\n",
    "import openpi.training.weight_loaders as _weight_loaders\n",
    "\n",
    "\n",
    "def init_logging():\n",
    "    \"\"\"Custom logging format for better readability.\"\"\"\n",
    "    level_mapping = {\n",
    "        \"DEBUG\": \"D\",\n",
    "        \"INFO\": \"I\",\n",
    "        \"WARNING\": \"W\",\n",
    "        \"ERROR\": \"E\",\n",
    "        \"CRITICAL\": \"C\",\n",
    "    }\n",
    "\n",
    "    class CustomFormatter(logging.Formatter):\n",
    "\n",
    "        def format(self, record):\n",
    "            record.levelname = level_mapping.get(record.levelname, record.levelname)\n",
    "            return super().format(record)\n",
    "\n",
    "    formatter = CustomFormatter(\n",
    "        fmt=\"%(asctime)s.%(msecs)03d [%(levelname)s] %(message)-80s (%(process)d:%(filename)s:%(lineno)s)\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers[0].setFormatter(formatter)\n",
    "\n",
    "\n",
    "def init_wandb(\n",
    "    config: _config.TrainConfig,\n",
    "    *,\n",
    "    resuming: bool,\n",
    "    log_code: bool = False,\n",
    "    enabled: bool = True,\n",
    "):\n",
    "    if not enabled:\n",
    "        wandb.init(mode=\"disabled\")\n",
    "        return\n",
    "\n",
    "    ckpt_dir = config.checkpoint_dir\n",
    "    if not ckpt_dir.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint directory {ckpt_dir} does not exist.\")\n",
    "    if resuming:\n",
    "        run_id = (ckpt_dir / \"wandb_id.txt\").read_text().strip()\n",
    "        wandb.init(id=run_id, resume=\"must\", project=config.project_name)\n",
    "    else:\n",
    "        wandb.init(\n",
    "            name=config.exp_name,\n",
    "            config=dataclasses.asdict(config),\n",
    "            project=config.project_name,\n",
    "        )\n",
    "        (ckpt_dir / \"wandb_id.txt\").write_text(wandb.run.id)\n",
    "\n",
    "    if log_code:\n",
    "        wandb.run.log_code(epath.Path(__file__).parent.parent)\n",
    "\n",
    "\n",
    "def _load_weights_and_validate(loader: _weight_loaders.WeightLoader, params_shape: at.Params) -> at.Params:\n",
    "    \"\"\"Loads and validates the weights. Returns a loaded subset of the weights.\"\"\"\n",
    "    loaded_params = loader.load(params_shape)\n",
    "    at.check_pytree_equality(expected=params_shape, got=loaded_params, check_shapes=True, check_dtypes=True)\n",
    "\n",
    "    # Remove jax.ShapeDtypeStruct from the loaded params. This makes sure that only the loaded params are returned.\n",
    "    return traverse_util.unflatten_dict({\n",
    "        k: v\n",
    "        for k, v in traverse_util.flatten_dict(loaded_params).items() if not isinstance(v, jax.ShapeDtypeStruct)\n",
    "    })\n",
    "\n",
    "\n",
    "@at.typecheck\n",
    "def init_train_state(\n",
    "    config: _config.TrainConfig,\n",
    "    init_rng: at.KeyArrayLike,\n",
    "    mesh: jax.sharding.Mesh,\n",
    "    *,\n",
    "    resume: bool,\n",
    ") -> tuple[training_utils.TrainState, Any]:\n",
    "    tx = _optimizer.create_optimizer(config.optimizer, config.lr_schedule, weight_decay_mask=None)\n",
    "\n",
    "    def init(rng: at.KeyArrayLike, partial_params: at.Params | None = None) -> training_utils.TrainState:\n",
    "        rng, model_rng = jax.random.split(rng)\n",
    "        # initialize the model (and its parameters).\n",
    "        model = config.model.create(model_rng)\n",
    "\n",
    "        # Merge the partial params into the model.\n",
    "        if partial_params is not None:\n",
    "            graphdef, state = nnx.split(model)\n",
    "            # This will produce an error if the partial params are not a subset of the state.\n",
    "            state.replace_by_pure_dict(partial_params)\n",
    "            model = nnx.merge(graphdef, state)\n",
    "\n",
    "        params = nnx.state(model)\n",
    "        # Convert frozen params to bfloat16.\n",
    "        params = nnx_utils.state_map(\n",
    "            params,\n",
    "            config.freeze_filter,\n",
    "            lambda p: p.replace(p.value.astype(jnp.bfloat16)),\n",
    "        )\n",
    "\n",
    "        return training_utils.TrainState(\n",
    "            step=0,\n",
    "            params=params,\n",
    "            model_def=nnx.graphdef(model),\n",
    "            tx=tx,\n",
    "            opt_state=tx.init(params.filter(config.trainable_filter)),\n",
    "            ema_decay=config.ema_decay,\n",
    "            ema_params=None if config.ema_decay is None else params,\n",
    "        )\n",
    "\n",
    "    train_state_shape = jax.eval_shape(init, init_rng)\n",
    "    state_sharding = sharding.fsdp_sharding(train_state_shape, mesh, log=True)\n",
    "\n",
    "    if resume:\n",
    "        return train_state_shape, state_sharding\n",
    "\n",
    "    partial_params = _load_weights_and_validate(config.weight_loader, train_state_shape.params.to_pure_dict())\n",
    "    replicated_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())\n",
    "\n",
    "    # Initialize the train state and mix in the partial params.\n",
    "    train_state = jax.jit(\n",
    "        init,\n",
    "        donate_argnums=(1, ),  # donate the partial params buffer.\n",
    "        in_shardings=replicated_sharding,\n",
    "        out_shardings=state_sharding,\n",
    "    )(init_rng, partial_params)\n",
    "\n",
    "    return train_state, state_sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf821a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
